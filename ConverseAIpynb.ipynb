{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install yt-dlp pydub datasets soundfile\n",
        "!pip install openai-whisper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeVxh_RQBx4G",
        "outputId": "993121f1-6560-41be-ced3-be5e71414037"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yt-dlp\n",
            "  Downloading yt_dlp-2025.8.27-py3-none-any.whl.metadata (175 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/176.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m176.0/176.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydub in /usr/local/lib/python3.12/dist-packages (0.25.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.12/dist-packages (4.0.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.12/dist-packages (0.13.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from datasets) (3.19.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.12/dist-packages (from datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.12/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.12/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.12/dist-packages (from datasets) (0.34.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from datasets) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.12/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.12/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.8)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.32.2->datasets) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading yt_dlp-2025.8.27-py3-none-any.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: yt-dlp\n",
            "Successfully installed yt-dlp-2025.8.27\n",
            "Collecting openai-whisper\n",
            "  Downloading openai_whisper-20250625.tar.gz (803 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m803.2/803.2 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (10.7.0)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.0.2)\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (0.11.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (2.8.0+cu126)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (4.67.1)\n",
            "Requirement already satisfied: triton>=2 in /usr/local/lib/python3.12/dist-packages (from openai-whisper) (3.4.0)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.12/dist-packages (from triton>=2->openai-whisper) (75.2.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba->openai-whisper) (0.43.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken->openai-whisper) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (4.15.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->openai-whisper) (1.11.1.6)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.8.3)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20250625-py3-none-any.whl size=803979 sha256=cd09f335b5bdeb0d00fdfa9ae9d42d04c20e43124ef7cd7a81e2bf573aefd36c\n",
            "  Stored in directory: /root/.cache/pip/wheels/61/d2/20/09ec9bef734d126cba375b15898010b6cc28578d8afdde5869\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: openai-whisper\n",
            "Successfully installed openai-whisper-20250625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, json\n",
        "import yt_dlp\n",
        "import whisper\n",
        "import nltk\n",
        "import torchaudio\n",
        "from datasets import Dataset, Audio\n",
        "from huggingface_hub import HfApi, HfFolder, Repository\n",
        "\n",
        "nltk.download(\"punkt\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYd1p4I5FPWf",
        "outputId": "070b7ae1-275f-48f7-b5ab-81717d5b8637"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yt_dlp\n",
        "from google.colab import files\n",
        "\n",
        "# Create folder for audio\n",
        "os.makedirs(\"audio_files\", exist_ok=True)\n",
        "\n",
        "choice = input(\"Enter '1' to upload a file manually OR '2' to provide a YouTube link: \")\n",
        "\n",
        "audio_files = []\n",
        "\n",
        "if choice.strip() == \"1\":\n",
        "    print(\"üì§ Please upload your audio file (mp3, wav, m4a supported)\")\n",
        "    uploaded = files.upload()\n",
        "    for filename in uploaded.keys():\n",
        "        dst_path = os.path.join(\"audio_files\", filename)\n",
        "        os.rename(filename, dst_path)\n",
        "        audio_files.append(dst_path)\n",
        "\n",
        "elif choice.strip() == \"2\":\n",
        "    url = input(\"üîó Enter YouTube link: \").strip()\n",
        "\n",
        "    ydl_opts = {\n",
        "        \"format\": \"bestaudio/best\",\n",
        "        \"outtmpl\": \"audio_files/%(id)s.%(ext)s\",\n",
        "        \"postprocessors\": [\n",
        "            {\n",
        "                \"key\": \"FFmpegExtractAudio\",\n",
        "                \"preferredcodec\": \"wav\",\n",
        "                \"preferredquality\": \"192\",\n",
        "            }\n",
        "        ],\n",
        "    }\n",
        "\n",
        "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "        info = ydl.extract_info(url, download=True)\n",
        "        filename = ydl.prepare_filename(info)\n",
        "        audio_path = os.path.splitext(filename)[0] + \".wav\"\n",
        "        audio_files.append(audio_path)\n",
        "\n",
        "else:\n",
        "    print(\"‚ùå Invalid choice. Please restart and select 1 or 2.\")\n",
        "\n",
        "print(\"‚úÖ Audio files ready:\", audio_files)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pGY09BTAC2tq",
        "outputId": "0cb69bd8-6320-4660-9682-ba5493d56e76"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter '1' to upload a file manually OR '2' to provide a YouTube link: 2\n",
            "üîó Enter YouTube link: https://youtu.be/1DOqousyDjc?si=9i3wN1SAdg9ozM7Q\n",
            "[youtube] Extracting URL: https://youtu.be/1DOqousyDjc?si=9i3wN1SAdg9ozM7Q\n",
            "[youtube] 1DOqousyDjc: Downloading webpage\n",
            "[youtube] 1DOqousyDjc: Downloading tv simply player API JSON\n",
            "[youtube] 1DOqousyDjc: Downloading tv client config\n",
            "[youtube] 1DOqousyDjc: Downloading player 6742b2b9-main\n",
            "[youtube] 1DOqousyDjc: Downloading tv player API JSON\n",
            "[info] 1DOqousyDjc: Downloading 1 format(s): 251\n",
            "[download] Destination: audio_files/1DOqousyDjc.webm\n",
            "[download] 100% of    4.58MiB in 00:00:00 at 25.37MiB/s  \n",
            "[ExtractAudio] Destination: audio_files/1DOqousyDjc.wav\n",
            "Deleting original file audio_files/1DOqousyDjc.webm (pass -k to keep)\n",
            "‚úÖ Audio files ready: ['audio_files/1DOqousyDjc.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "files = [f for f in os.listdir(\"audio_files\") if f.endswith(\".wav\")]\n",
        "print(\"Audio files found:\", files)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N91RvvWhI-e2",
        "outputId": "a09ed1b4-77fb-478c-8869-799ada643ada"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio files found: ['1DOqousyDjc.wav']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "all_transcriptions = []\n",
        "\n",
        "for f in os.listdir(\"audio_files\"):\n",
        "    if f.endswith((\".wav\",\".mp3\")):\n",
        "        result = model.transcribe(f\"audio_files/{f}\")\n",
        "        all_transcriptions.append({\n",
        "            \"file\": f,\n",
        "            \"text\": result[\"text\"],\n",
        "            \"segments\": result[\"segments\"]\n",
        "        })\n",
        "\n",
        "with open(\"transcriptions.json\", \"w\") as f:\n",
        "    json.dump(all_transcriptions, f, indent=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cH8_3stFhAy",
        "outputId": "5160819b-d782-4cce-fe5e-cc446a86ea01"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")   # NEW\n",
        "\n",
        "sentences = []\n",
        "for entry in all_transcriptions:\n",
        "    for seg in entry[\"segments\"]:\n",
        "        for s in nltk.sent_tokenize(seg[\"text\"]):\n",
        "            sentences.append({\"file\": entry[\"file\"], \"sentence\": s})\n",
        "\n",
        "with open(\"sentences.json\", \"w\") as f:\n",
        "    json.dump(sentences, f, indent=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MWrE2xvZFk4g",
        "outputId": "8dd0fe21-e667-453c-cf36-75956f9a30f3"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "clip_dataset = []\n",
        "clip_text = \"\"\n",
        "clip_time = 0\n",
        "clip_id = 0\n",
        "\n",
        "max_duration = 30  # seconds\n",
        "\n",
        "for entry in all_transcriptions:\n",
        "    waveform, sr = torchaudio.load(f\"raw_audio/{entry['file']}\")\n",
        "\n",
        "    for seg in entry[\"segments\"]:\n",
        "        seg_duration = seg[\"end\"] - seg[\"start\"]\n",
        "        if clip_time + seg_duration <= max_duration:\n",
        "            clip_text += \" \" + seg[\"text\"]\n",
        "            clip_time += seg_duration\n",
        "        else:\n",
        "            clip_dataset.append({\n",
        "                \"id\": f\"clip_{clip_id}\",\n",
        "                \"text\": clip_text.strip()\n",
        "            })\n",
        "            clip_id += 1\n",
        "            clip_text = seg[\"text\"]\n",
        "            clip_time = seg_duration\n",
        "\n",
        "# save last\n",
        "if clip_text:\n",
        "    clip_dataset.append({\"id\": f\"clip_{clip_id}\", \"text\": clip_text.strip()})\n",
        "\n",
        "with open(\"clips.json\", \"w\") as f:\n",
        "    json.dump(clip_dataset, f, indent=2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "Q8YtvsvdFn3l",
        "outputId": "fe646cdb-f0bb-418d-972a-6b7548e5ff70"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to open the input \"raw_audio/1DOqousyDjc.wav\" (No such file or directory).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1034648307.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_transcriptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"raw_audio/{entry['file']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"segments\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_internal/module_utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf' It will be removed from {\"a future\" if version is None else \"the \" + str(version)} release. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"raw_audio/1DOqousyDjc.wav\" (No such file or directory)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, torchaudio\n",
        "\n",
        "os.makedirs(\"clips_audio\", exist_ok=True)\n",
        "new_dataset = []\n",
        "\n",
        "clip_id = 0\n",
        "for entry in all_transcriptions:\n",
        "    wav, sr = torchaudio.load(f\"raw_audio/{entry['file']}\")\n",
        "\n",
        "    clip_text = \"\"\n",
        "    clip_start = 0.0\n",
        "    clip_end = 0.0\n",
        "\n",
        "    for seg in entry[\"segments\"]:\n",
        "        seg_dur = seg[\"end\"] - seg[\"start\"]\n",
        "        if (clip_end - clip_start) + seg_dur <= 30:\n",
        "            # accumulate text\n",
        "            clip_text += \" \" + seg[\"text\"]\n",
        "            clip_end = seg[\"end\"]\n",
        "        else:\n",
        "            # save clip audio\n",
        "            filename = f\"clips_audio/clip_{clip_id}.wav\"\n",
        "            start_frame = int(clip_start * sr)\n",
        "            end_frame = int(clip_end * sr)\n",
        "            torchaudio.save(filename, wav[:, start_frame:end_frame], sr)\n",
        "\n",
        "            new_dataset.append({\"path\": filename, \"text\": clip_text.strip()})\n",
        "\n",
        "            clip_id += 1\n",
        "            # reset for next\n",
        "            clip_text = seg[\"text\"]\n",
        "            clip_start = seg[\"start\"]\n",
        "            clip_end = seg[\"end\"]\n",
        "\n",
        "# save last clip\n",
        "if clip_text:\n",
        "    filename = f\"clips_audio/clip_{clip_id}.wav\"\n",
        "    start_frame = int(clip_start * sr)\n",
        "    end_frame = int(clip_end * sr)\n",
        "    torchaudio.save(filename, wav[:, start_frame:end_frame], sr)\n",
        "    new_dataset.append({\"path\": filename, \"text\": clip_text.strip()})\n",
        "\n",
        "# Build HF dataset with audio + text\n",
        "from datasets import Dataset, Audio\n",
        "\n",
        "ds = Dataset.from_list(new_dataset)\n",
        "ds = ds.cast_column(\"path\", Audio())\n",
        "ds.save_to_disk(\"processed_dataset\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "D-xU1SXyFpsS",
        "outputId": "74d9f9f2-728c-4f65-ccac-f2e0562e0ed0"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Failed to open the input \"raw_audio/1DOqousyDjc.wav\" (No such file or directory).",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2658578713.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mclip_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mall_transcriptions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mwav\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"raw_audio/{entry['file']}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mclip_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    220\u001b[0m         )\n\u001b[1;32m    221\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torchaudio/_internal/module_utils.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf' It will be removed from {\"a future\" if version is None else \"the \" + str(version)} release. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Failed to open the input \"raw_audio/1DOqousyDjc.wav\" (No such file or directory)."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# üîë Login (paste your HF token when prompted)\n",
        "login()\n",
        "\n",
        "# üìÇ Load the dataset from disk\n",
        "ds = load_from_disk(\"processed_dataset\")\n",
        "\n",
        "# üÜï Change this to your own repo name\n",
        "repo_id = \"nty23/fine\"\n",
        "\n",
        "# Push to Hub\n",
        "ds.push_to_hub(repo_id)\n"
      ],
      "metadata": {
        "id": "BNjSFNNjFr4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Audio\n",
        "\n",
        "files = [os.path.join(\"audio_files\", f) for f in os.listdir(\"audio_files\") if f.endswith(\".wav\")]\n",
        "\n",
        "if len(files) == 0:\n",
        "    raise ValueError(\"‚ùå No audio files found. Please upload or download again.\")\n",
        "\n",
        "dataset = Dataset.from_dict({\"audio\": files})\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "7TtCJP7MJEaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell 9 ‚Äî flatten processed_dataset to plain strings (path, text)\n",
        "from datasets import load_from_disk, Dataset\n",
        "import os\n",
        "\n",
        "# Load the saved dataset\n",
        "ds_raw = load_from_disk(\"processed_dataset\")\n",
        "\n",
        "# SAFELY extract filepaths from the Arrow struct without decoding audio\n",
        "# (We avoid .map() so nothing tries to auto-decode)\n",
        "paths = []\n",
        "texts = []\n",
        "table = ds_raw.data  # this is a pyarrow.Table\n",
        "\n",
        "# The \"path\" column is a struct<bytes: binary, path: string>. We want its \"path\" field only.\n",
        "for chunk in table.column(\"path\").chunks:\n",
        "    # chunk is a pyarrow.StructArray\n",
        "    # get the \"path\" field (string) and extend to python list\n",
        "    paths.extend(chunk.field(\"path\").to_pylist())\n",
        "\n",
        "# text column is plain string; gather all rows\n",
        "for chunk in table.column(\"text\").chunks:\n",
        "    texts.extend(chunk.to_pylist())\n",
        "\n",
        "assert len(paths) == len(texts), \"Mismatch between paths and texts length.\"\n",
        "\n",
        "# Build a clean list of dicts (no Audio feature anywhere)\n",
        "clean_list = [{\"path\": p, \"text\": t} for p, t in zip(paths, texts)]\n",
        "\n",
        "# Rebuild a fresh HF dataset with ONLY strings\n",
        "ds = Dataset.from_list(clean_list)\n",
        "\n",
        "print(ds)\n",
        "print(\"Example row:\", ds[0])\n",
        "print(\"‚úÖ Dataset rebuilt with plain strings only.\")\n"
      ],
      "metadata": {
        "id": "LrwFP0k4Z_Xd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Audio\n",
        "\n",
        "# Collect downloaded wav files\n",
        "files = [os.path.join(\"audio_files\", f) for f in os.listdir(\"audio_files\") if f.endswith(\".wav\")]\n",
        "\n",
        "dataset = Dataset.from_dict({\"audio\": files})\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print(dataset)\n"
      ],
      "metadata": {
        "id": "-HrLLUuugmMS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers accelerate librosa"
      ],
      "metadata": {
        "id": "A_mEl0m5KE_5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/m-bain/whisperx.git\n"
      ],
      "metadata": {
        "id": "8CEf1JkFCLFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load Whisper pipeline with word timestamps\n",
        "pipe = pipeline(\n",
        "    \"automatic-speech-recognition\",\n",
        "    model=\"openai/whisper-small\",  # small for speed, can change to base/medium/large\n",
        "    generate_kwargs={\"task\": \"transcribe\", \"return_timestamps\": \"word\"}\n",
        ")"
      ],
      "metadata": {
        "id": "e1shqf9XKM3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import whisperx\n",
        "import torch\n",
        "from datasets import load_from_disk\n",
        "\n",
        "# Load cleaned dataset\n",
        "cleaned_ds = load_from_disk(\"cleaned_dataset\")\n",
        "\n",
        "# Load model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = whisperx.load_model(\"small\", device)\n",
        "\n",
        "# Alignment model for word-level timestamps\n",
        "align_model, metadata = whisperx.load_align_model(language_code=\"en\", device=device)\n",
        "\n",
        "def transcribe_with_timestamps(batch):\n",
        "    try:\n",
        "        import numpy as np\n",
        "        audio_array = np.array(batch[\"array\"], dtype=np.float32)\n",
        "\n",
        "        # Run ASR\n",
        "        result = model.transcribe(audio_array, batch_size=8)\n",
        "\n",
        "        # Align to word-level\n",
        "        result_aligned = whisperx.align(result[\"segments\"], align_model, metadata, audio_array, device)\n",
        "\n",
        "        batch[\"transcription\"] = result[\"text\"]\n",
        "        batch[\"segments\"] = result_aligned[\"segments\"]   # sentence-level with start/end\n",
        "        batch[\"words\"] = result_aligned[\"word_segments\"] # word-level with start/end\n",
        "    except Exception as e:\n",
        "        batch[\"transcription\"] = f\"‚ùå Error: {e}\"\n",
        "        batch[\"segments\"] = []\n",
        "        batch[\"words\"] = []\n",
        "    return batch\n",
        "\n",
        "# Map over dataset\n",
        "transcribed_ds = cleaned_ds.map(transcribe_with_timestamps)\n",
        "\n",
        "# Save\n",
        "transcribed_ds.save_to_disk(\"transcribed_dataset\")\n",
        "print(\"üì¶ Saved transcribed_dataset with word-level timestamps.\")\n"
      ],
      "metadata": {
        "id": "UmWIWK50CO-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "transcriptions = []\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    # Get audio path from dataset\n",
        "    audio_path = dataset[i][\"audio\"][\"path\"]\n",
        "\n",
        "    # Load audio with librosa (16kHz mono)\n",
        "    audio_data, sr = librosa.load(audio_path, sr=16000, mono=True)\n",
        "\n",
        "    # Split into 30s chunks\n",
        "    chunk_size = sr * 30  # 30 seconds in samples\n",
        "    for start in range(0, len(audio_data), chunk_size):\n",
        "        chunk = audio_data[start:start + chunk_size]\n",
        "\n",
        "        # Transcribe each chunk (timestamps disabled)\n",
        "        result = pipe({\"array\": chunk, \"sampling_rate\": sr}, return_timestamps=False)\n",
        "        transcriptions.append(result[\"text\"])\n",
        "\n",
        "print(\"‚úÖ Transcription completed. Number of chunks processed:\", len(transcriptions))\n"
      ],
      "metadata": {
        "id": "8pDmroCtKS7S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "with open(\"transcriptions.json\", \"w\") as f:\n",
        "    json.dump(transcriptions, f, indent=2)\n",
        "\n",
        "print(\"üìÑ Transcriptions saved to transcriptions.json\")\n"
      ],
      "metadata": {
        "id": "DjxH7N9oKTEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def split_by_punctuation(words, punctuations=\".?!\"):\n",
        "    segments = []\n",
        "    current = []\n",
        "    for w in words:\n",
        "        current.append(w)\n",
        "        if any(w[\"word\"].endswith(p) for p in punctuations):\n",
        "            segments.append(current)\n",
        "            current = []\n",
        "    if current:\n",
        "        segments.append(current)\n",
        "    return segments\n",
        "\n",
        "# Example: split first transcription into sentences\n",
        "first_words = transcriptions[0][\"chunks\"]  # list of {word, timestamp}\n",
        "segments = split_by_punctuation(first_words)\n",
        "\n",
        "for idx, seg in enumerate(segments[:5]):\n",
        "    text = \" \".join([w[\"text\"] for w in seg])\n",
        "    start = seg[0][\"timestamp\"][0]\n",
        "    end = seg[-1][\"timestamp\"][1]\n",
        "    print(f\"Segment {idx+1}: {text} ({start:.2f}s ‚Üí {end:.2f}s)\")\n"
      ],
      "metadata": {
        "id": "BEy8DJGjKTG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Join all chunk transcriptions into one text\n",
        "full_transcript = \" \".join(transcriptions)\n",
        "\n",
        "print(\"‚úÖ Combined transcript length:\", len(full_transcript))\n",
        "print(\"üîπ Preview:\\n\", full_transcript[:500])  # show first 500 chars\n"
      ],
      "metadata": {
        "id": "y1S8-olIVwjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n"
      ],
      "metadata": {
        "id": "v7f4ZiCNWaKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Split transcript into sentences (NLTK handles punctuation intelligently)\n",
        "sentences = sent_tokenize(full_transcript)\n",
        "\n",
        "print(\"‚úÖ Transcript split into\", len(sentences), \"sentences\")\n",
        "print(\"üîπ First 5 sentences:\\n\", sentences[:5])\n"
      ],
      "metadata": {
        "id": "PPceI3PJWcom"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "word_chunks = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "print(\"‚úÖ Example word tokens from first sentence:\\n\", word_chunks[0])\n"
      ],
      "metadata": {
        "id": "myRVhu69Whn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth datasets transformers accelerate bitsandbytes\n",
        "\n"
      ],
      "metadata": {
        "id": "sFAdOtnmfpdv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, Audio\n",
        "from unsloth import FastLanguageModel\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "7McSGA8SfrBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Suppose you have audio file paths + corresponding transcripts\n",
        "# Example: audio_files = [\"audio1.wav\", \"audio2.wav\", ...]\n",
        "#          transcripts = [\"Hello world\", \"This is a test\", ...]\n",
        "\n",
        "dataset = Dataset.from_dict({\n",
        "    \"audio\": audio_files,   # list of file paths\n",
        "    \"text\": transcripts,    # aligned text\n",
        "})\n",
        "\n",
        "# Convert audio column into Hugging Face Audio type\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "print(dataset[0])\n"
      ],
      "metadata": {
        "id": "yKU68NVofrOr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"openai/whisper-small\",  # can also try \"tiny\" or \"base\"\n",
        "    max_seq_length = 512,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "0iRwlPx0frUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_batch(batch):\n",
        "    audio = batch[\"audio\"][\"array\"]\n",
        "    inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", truncation=True)\n",
        "    batch[\"input_ids\"] = inputs[\"input_ids\"][0]\n",
        "    batch[\"attention_mask\"] = inputs[\"attention_mask\"][0]\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(prepare_batch)\n"
      ],
      "metadata": {
        "id": "xxsqi1MYf3G5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset.select(range(2)),  # just 2 samples for quick sanity check\n",
        "    tokenizer=tokenizer,\n",
        "    max_steps=100,    # increase for real training\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "EF9e1s6Mf6Ix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"finetuned-whisper-unsloth\")\n",
        "tokenizer.save_pretrained(\"finetuned-whisper-unsloth\")\n"
      ],
      "metadata": {
        "id": "RKLGjGlzf82O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install unsloth datasets transformers accelerate bitsandbytes peft\n"
      ],
      "metadata": {
        "id": "4mAKBPz01emx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, Audio\n",
        "from unsloth import FastLanguageModel\n",
        "from peft import LoraConfig\n",
        "import torch\n"
      ],
      "metadata": {
        "id": "HtZRmGin1lAZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"openai/whisper-small\",  # try \"tiny\" or \"base\" if low VRAM\n",
        "    max_seq_length = 512,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "mqlZtefh1lFK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configure LoRA\n",
        "lora_config = LoraConfig(\n",
        "    r=16,                # rank\n",
        "    lora_alpha=32,       # scaling\n",
        "    target_modules=[\"q_proj\",\"v_proj\"],  # attention projection layers\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",  # Whisper is seq2seq\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(model, lora_config)\n"
      ],
      "metadata": {
        "id": "uPptX33k1lI4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = Dataset.from_dict({\n",
        "    \"audio\": audio_files,\n",
        "    \"text\": transcripts,\n",
        "})\n",
        "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "\n",
        "def prepare_batch(batch):\n",
        "    inputs = tokenizer(batch[\"text\"], return_tensors=\"pt\", truncation=True)\n",
        "    batch[\"input_ids\"] = inputs[\"input_ids\"][0]\n",
        "    batch[\"attention_mask\"] = inputs[\"attention_mask\"][0]\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(prepare_batch)\n"
      ],
      "metadata": {
        "id": "qLohrce01lMy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=dataset.select(range(2)),\n",
        "    tokenizer=tokenizer,\n",
        "    max_steps=200,    # increase for real training\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    logging_steps=10,\n",
        "    save_steps=50,\n",
        ")\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "XsLqyDkU1lQh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"whisper-lora-finetuned\")\n",
        "tokenizer.save_pretrained(\"whisper-lora-finetuned\")\n"
      ],
      "metadata": {
        "id": "IEmshqu11lUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "\n",
        "# Reload base + LoRA\n",
        "base_model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = \"openai/whisper-small\",\n",
        "    max_seq_length = 512,\n",
        "    dtype = torch.float16,\n",
        "    load_in_4bit = True,\n",
        ")\n",
        "\n",
        "model = PeftModel.from_pretrained(base_model, \"whisper-lora-finetuned\")\n",
        "\n",
        "# Test transcription\n",
        "inputs = tokenizer(\"dummy input\", return_tensors=\"pt\").to(\"cuda\")\n",
        "outputs = model.generate(**inputs, max_length=100)\n",
        "print(tokenizer.decode(outputs[0]))\n"
      ],
      "metadata": {
        "id": "_q3geRGx1xJc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# Shuffle before splitting (important!)\n",
        "dataset = dataset.shuffle(seed=42)\n",
        "\n",
        "# 90% train, 10% eval\n",
        "split_dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "train_dataset = split_dataset[\"train\"]\n",
        "eval_dataset  = split_dataset[\"test\"]\n",
        "\n",
        "print(train_dataset)\n",
        "print(eval_dataset)\n"
      ],
      "metadata": {
        "id": "fpQIvk5v2G60"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from unsloth import Trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    max_steps=200,   # increase for real runs\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,   # evaluate regularly\n",
        "    save_steps=50,\n",
        ")\n"
      ],
      "metadata": {
        "id": "t-q4F2wx2HwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "wer = evaluate.load(\"wer\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = logits.argmax(-1)\n",
        "    pred_str = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "    label_str = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "    return {\"wer\": wer.compute(predictions=pred_str, references=label_str)}\n",
        "\n",
        "trainer.compute_metrics = compute_metrics\n"
      ],
      "metadata": {
        "id": "hHUwD9Um2Js3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sesame-ai\n"
      ],
      "metadata": {
        "id": "7oOsn5UO4uTy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sesame import SesameTrainer\n"
      ],
      "metadata": {
        "id": "krT3FRtL4uYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sesame_trainer = SesameTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    max_steps=200,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    logging_steps=10,\n",
        "    eval_steps=50,\n",
        "    save_steps=50,\n",
        "    save_strategy=\"steps\",\n",
        "    evaluation_strategy=\"steps\",\n",
        ")\n"
      ],
      "metadata": {
        "id": "NK4_9Dm74ud_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sesame_trainer.train()\n"
      ],
      "metadata": {
        "id": "6uHGjXiR4ypB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sesame_trainer.save_model(\"whisper-lora-sesame\")\n",
        "tokenizer.save_pretrained(\"whisper-lora-sesame\")\n"
      ],
      "metadata": {
        "id": "Dl1xY_j94ywD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base model transcription\n",
        "base_text = base_pipe(audio_path)[\"text\"]\n",
        "\n",
        "# Fine-tuned model transcription\n",
        "fine_text = lora_pipe(audio_path)[\"text\"]\n",
        "\n",
        "print(\"Ground Truth:\", ground_truth_text)\n",
        "print(\"Base Whisper:\", base_text)\n",
        "print(\"Fine-tuned Whisper:\", fine_text)\n"
      ],
      "metadata": {
        "id": "nNT8Ctau4y7w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoProcessor, AutoModelForTextToWaveform\n",
        "import soundfile as sf\n",
        "import torch\n",
        "\n",
        "# Load model + processor\n",
        "processor = AutoProcessor.from_pretrained(\"coqui/XTTS-v2\")\n",
        "model = AutoModelForTextToWaveform.from_pretrained(\"coqui/XTTS-v2\").to(\"cuda\")\n",
        "\n",
        "# Use original speaker audio for cloning\n",
        "with open(audio_path, \"rb\") as f:\n",
        "    speaker_ref = f.read()\n",
        "\n",
        "def synthesize_tts(text, ref_audio, filename):\n",
        "    inputs = processor(text=text, speaker_prompt=ref_audio, sampling_rate=22050, return_tensors=\"pt\")\n",
        "    with torch.no_grad():\n",
        "        audio = model(**inputs).waveform\n",
        "    sf.write(filename, audio.cpu().numpy().squeeze(), 22050)\n",
        "\n",
        "# Generate\n",
        "synthesize_tts(base_text, speaker_ref, \"base_tts.wav\")\n",
        "synthesize_tts(fine_text, speaker_ref, \"fine_tts.wav\")\n"
      ],
      "metadata": {
        "id": "ZqtHF76c4y_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.pretrained import EncoderClassifier\n",
        "import torchaudio\n",
        "\n",
        "classifier = EncoderClassifier.from_hparams(\n",
        "    source=\"speechbrain/spkrec-ecapa-voxceleb\"\n",
        ")\n",
        "\n",
        "def get_embedding(path):\n",
        "    signal, fs = torchaudio.load(path)\n",
        "    embedding = classifier.encode_batch(signal)\n",
        "    return embedding.mean(dim=1)\n",
        "\n",
        "# Compute embeddings\n",
        "emb_real = get_embedding(\"real_voice.wav\")\n",
        "emb_base = get_embedding(\"base_tts.wav\")\n",
        "emb_fine = get_embedding(\"fine_tts.wav\")\n",
        "\n",
        "# Cosine similarities\n",
        "import torch\n",
        "sim_base = torch.cosine_similarity(emb_real, emb_base).item()\n",
        "sim_fine = torch.cosine_similarity(emb_real, emb_fine).item()\n",
        "\n",
        "print(f\"Similarity (Real vs Base TTS): {sim_base:.3f}\")\n",
        "print(f\"Similarity (Real vs Fine-tuned TTS): {sim_fine:.3f}\")\n"
      ],
      "metadata": {
        "id": "5aREekUh4zDH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers\n",
        "!pip install soundfile torchaudio speechbrain accelerate\n",
        "!pip install git+https://github.com/coqui-ai/TTS\n"
      ],
      "metadata": {
        "id": "jvnXC3yh-hL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchaudio\n",
        "import soundfile as sf\n",
        "from transformers import pipeline\n",
        "\n",
        "# Paths\n",
        "audio_path = \"real_voice.wav\"  # replace with your uploaded/recorded file\n",
        "\n",
        "# Load Whisper base\n",
        "base_pipe = pipeline(\"automatic-speech-recognition\", model=\"openai/whisper-base\", device=0)\n",
        "\n",
        "# Load fine-tuned (LoRA + Sesame) Whisper\n",
        "# Assuming you've saved it after training with Unsloth\n",
        "fine_pipe = pipeline(\"automatic-speech-recognition\", model=\"./finetuned_whisper\", device=0)\n"
      ],
      "metadata": {
        "id": "Qaf27X0B-w95"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Base transcription\n",
        "base_text = base_pipe(audio_path)[\"text\"]\n",
        "\n",
        "# Fine-tuned transcription\n",
        "fine_text = fine_pipe(audio_path)[\"text\"]\n",
        "\n",
        "print(\"Base Whisper:\", base_text)\n",
        "print(\"Fine-tuned Whisper:\", fine_text)\n"
      ],
      "metadata": {
        "id": "IJYd3rOx-w5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from TTS.api import TTS\n",
        "\n",
        "# Load XTTS model (multilingual + voice cloning)\n",
        "tts = TTS(\"tts_models/multilingual/multi-dataset/xtts_v2\").to(\"cuda\")\n",
        "\n",
        "# Clone voice and synthesize\n",
        "tts.tts_to_file(text=base_text, speaker_wav=audio_path, file_path=\"base_clone.wav\")\n",
        "tts.tts_to_file(text=fine_text, speaker_wav=audio_path, file_path=\"fine_clone.wav\")\n"
      ],
      "metadata": {
        "id": "W1Kj3H8v-w1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from speechbrain.pretrained import EncoderClassifier\n",
        "\n",
        "# Load speaker embedding model\n",
        "classifier = EncoderClassifier.from_hparams(source=\"speechbrain/spkrec-ecapa-voxceleb\")\n",
        "\n",
        "def get_embedding(path):\n",
        "    signal, fs = torchaudio.load(path)\n",
        "    embedding = classifier.encode_batch(signal)\n",
        "    return embedding.mean(dim=1)\n",
        "\n",
        "# Get embeddings\n",
        "emb_real = get_embedding(audio_path)\n",
        "emb_base = get_embedding(\"base_clone.wav\")\n",
        "emb_fine = get_embedding(\"fine_clone.wav\")\n",
        "\n",
        "# Cosine similarity\n",
        "sim_base = torch.cosine_similarity(emb_real, emb_base).item()\n",
        "sim_fine = torch.cosine_similarity(emb_real, emb_fine).item()\n",
        "\n",
        "print(f\"Similarity (Real vs Base Clone): {sim_base:.3f}\")\n",
        "print(f\"Similarity (Real vs Fine Clone): {sim_fine:.3f}\")\n"
      ],
      "metadata": {
        "id": "qliYJD0P-wxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import IPython.display as ipd\n",
        "\n",
        "print(\"üîä Real Voice:\")\n",
        "display(ipd.Audio(audio_path))\n",
        "\n",
        "print(\"üîä Base Whisper Clone:\")\n",
        "display(ipd.Audio(\"base_clone.wav\"))\n",
        "\n",
        "print(\"üîä Fine-tuned Whisper Clone:\")\n",
        "display(ipd.Audio(\"fine_clone.wav\"))\n"
      ],
      "metadata": {
        "id": "jrcRG9Q3-wtD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7O9uZg1u-wme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IV5qoGFb-wiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_q01jCFs-weN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bQmvJpku-waB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OSqxJWlC-wU-"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}